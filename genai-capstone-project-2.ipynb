{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11383350,"sourceType":"datasetVersion","datasetId":7127736}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Board Game Rule Q&A Using Retrieval Augmented Generation (RAG)\n-- A project by Rutuja Jangle\n\n## Project Overview\nThis project demonstrates a Retrieval Augmented Generation (RAG) application tailored to help users quickly understand board game rules by applying several Gen AI capabilities. The project transforms lengthy board game rule PDFs into manageable text chunks, creates embeddings for each chunk, and uses a custom retrieval process (via cosine similarity–based vector search) to find the most relevant information in response to a user query. Finally, it synthesizes an answer using Google Generative AI's text generation API.\n","metadata":{}},{"cell_type":"markdown","source":"## Use Case and Innovation\n\n### Use Case\n**Problem:** Many board games come with extensive, complex rulebooks that can overwhelm new players. Finding the answer to a specific rule-related query often means manually searching through multiple pages of dense text.\n\n**Solution:**  \n- Automatically extract and split the text from PDF rulebooks (document understanding).  \n- Converting text chunks into semantic vector representations (embeddings).  \n- Retrieving the most relevant segments through a custom retrieval algorithm (RAG).\n- Synthesizing a concise answer based on the retrieved context (controlled generation).\n\n### Innovation & Impact\n- **Novel Approach:** The pipeline leverages state-of-the-art Gen AI capabilities in a novel way by integrating document understanding, semantic embeddings, and retrieval augmented generation.  \n- **Impact:** The solution helps lower the learning curve for board games by providing quick answers derived from official rulebooks. While focused on board game rules, the underlying pipeline (PDF ingestion, embedding generation, and RAG) is highly generic and can be repurposed for any domain that involves dense textual documents (legal texts, manuals, research articles).  ","metadata":{}},{"cell_type":"markdown","source":"## Environment Setup & Data Discovery\n\nBefore we start processing rulebook PDFs, we need to prepare our environment and verify what data is available:\n\n- **Import core libraries**  \n  - `numpy` (`np`): for efficient numerical operations (e.g., vector math later)  \n  - `pandas` (`pd`): for structured data handling (e.g., tabular metadata or results)\n\n- **Inspect the Kaggle input directory**  \n  - We use Python’s `os.walk()` to recursively traverse `/kaggle/input`,  \n  - Printing each file path lets us confirm that all PDF rulebooks (or other assets) are present and correctly mounted.\n\nEnsuring the input files are accessible is a crucial first step in our RAG pipeline—if the raw documents aren’t there, nothing else can proceed!  \n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:24:15.615671Z","iopub.execute_input":"2025-04-15T00:24:15.615976Z","iopub.status.idle":"2025-04-15T00:24:16.151397Z","shell.execute_reply.started":"2025-04-15T00:24:15.615953Z","shell.execute_reply":"2025-04-15T00:24:16.150396Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks/Corrosion rulebook.pdf\n/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks/Metal Gear rulebook.pdf\n/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks/The Thing rulebook.pdf\n/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks/Intarsia rulebook.pdf\n/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks/Azul rulebook.pdf\n/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks/PointCity rulebook.pdf\n/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks/Eldritch rulebook.pdf\n/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks/Chess rulebook.pdf\n/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks/Kick-Ass rulebook.pdf\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Setup & Dependency Installation\n\nBefore diving into the RAG pipeline, we need to install and pin all the required libraries for the end‑to‑end RAG pipeline, everything from PDF parsing and vector search to the Google GenAI SDK (Gemini). After the installs, it imports the SDK and prints its version to confirm that we’re working with the exact client version required for embedding generation and answer synthesis.  \n","metadata":{}},{"cell_type":"code","source":"\n!pip install \"jupyterlab>=3.1.0,<4.0.0\"\n\n\n!pip install protobuf==3.19.6\n\n\n!pip install \"google-api-core>=2.19.1,<3.0.0\"\n\n\n!pip install \"rich<14\"\n\n\n!pip install PyPDF2 faiss-cpu openai tqdm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -qqy google-genai\n!pip install -qU google-genai==1.7.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:24:21.076861Z","iopub.execute_input":"2025-04-15T00:24:21.077304Z","iopub.status.idle":"2025-04-15T00:24:27.215604Z","shell.execute_reply.started":"2025-04-15T00:24:21.077280Z","shell.execute_reply":"2025-04-15T00:24:27.213976Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:24:35.675225Z","iopub.execute_input":"2025-04-15T00:24:35.675572Z","iopub.status.idle":"2025-04-15T00:24:36.593489Z","shell.execute_reply.started":"2025-04-15T00:24:35.675544Z","shell.execute_reply":"2025-04-15T00:24:36.592391Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Setting up API key\n\nI used an API key generated from the Google AI Studio and then stored it in Kaggle secrets inside the Add-ons menu and named it as `GOOGLE_API_KEY`","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\n# Retrieve your API key\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\n# Configure the client with your API key\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:27:25.611527Z","iopub.execute_input":"2025-04-15T00:27:25.611876Z","iopub.status.idle":"2025-04-15T00:27:26.320115Z","shell.execute_reply.started":"2025-04-15T00:27:25.611852Z","shell.execute_reply":"2025-04-15T00:27:26.318897Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Initializing GenAI Client & Listing Embedding Models\n\nHere I have first created my GenAI client with my API key and then listed all models that support `embedContent` API method so I can pick the best one for embedding in my RAG pipeline.  \n","metadata":{}},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nfor m in client.models.list():\n    if m.supported_actions is not None and \"embedContent\" in m.supported_actions:\n        print(m.name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:24:54.921688Z","iopub.execute_input":"2025-04-15T00:24:54.922007Z","iopub.status.idle":"2025-04-15T00:24:55.325762Z","shell.execute_reply.started":"2025-04-15T00:24:54.921983Z","shell.execute_reply":"2025-04-15T00:24:55.324574Z"}},"outputs":[{"name":"stdout","text":"models/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Data Ingestion: Extracting & Loading Rulebook PDFs\n\n \nIn this cell, I pointed to my input folder where all the board game rulebook PDFs are stored and used PyPDF2 to pull the raw text out of each file. First, I defined an `extract_text_from_pdf()` helper that opens a PDF in binary mode, loops through every page, and safely grabs whatever text is there (falling back to an empty string if a page is blank or extraction fails). Then I walked the directory to find every `.pdf`, run it through that function, and build a list of dictionaries, each with the original filename under `source` and the full extracted text under `content`. Finally, I printed how many documents loaded successfully so I know my RAG pipeline has the raw rulebook text ready for chunking, embedding, and retrieval.  \n","metadata":{}},{"cell_type":"code","source":"import os\nimport PyPDF2\nfrom IPython.display import Markdown\n\nPDF_FOLDER = \"/kaggle/input/boardgame-rulebooks/BoardGame_Rulebooks\"\n\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"\n    Extract text from a PDF using PyPDF2.\n    \"\"\"\n    text = []\n    try:\n        with open(pdf_path, \"rb\") as f:\n            reader = PyPDF2.PdfReader(f)\n            for page in reader.pages:\n                page_text = page.extract_text() or \"\"\n                text.append(page_text)\n    except Exception as e:\n        print(f\"Error reading {pdf_path}: {e}\")\n    \n    return \"\\n\".join(text)\n\n# Load all PDFs in the folder\ndocuments = []\nfor file_name in os.listdir(PDF_FOLDER):\n    if file_name.lower().endswith(\".pdf\"):\n        full_path = os.path.join(PDF_FOLDER, file_name)\n        pdf_text = extract_text_from_pdf(full_path)\n        if pdf_text:\n            documents.append({\n                \"source\": file_name,\n                \"content\": pdf_text\n            })\n\nprint(f\"Loaded {len(documents)} PDF documents.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:24:57.934920Z","iopub.execute_input":"2025-04-15T00:24:57.935429Z","iopub.status.idle":"2025-04-15T00:25:47.940331Z","shell.execute_reply.started":"2025-04-15T00:24:57.935390Z","shell.execute_reply":"2025-04-15T00:25:47.938766Z"}},"outputs":[{"name":"stdout","text":"Loaded 9 PDF documents.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Text chunking\n\nHere, I just broke down each rulebook’s full text into bite‑sized pieces for embedding and retrieval. I started with a simple `split_into_chunks()` function that splits any long string into segments of roughly 300 words by counting space‑delimited tokens. Then, I looped over every document I loaded earlier, running its content through that splitter, and collected each segment into `all_chunks`, annotating it with the original filename (`source`), the chunk text itself, and a unique `chunk_id` (filename plus chunk index). Finally, I printed the total number of chunks to confirm that my RAG pipeline has a nicely partitioned set of passages to embed and search over. This was done because splitting the text improves both embedding quality and retrieval performance.\n","metadata":{}},{"cell_type":"code","source":"def split_into_chunks(text, max_words=300):\n    \"\"\"\n    Naive splitting by word count.\n    Splits the text into segments of ~300 words each.\n    \"\"\"\n    words = text.split()\n    chunks = []\n    current = []\n    \n    for w in words:\n        current.append(w)\n        if len(current) >= max_words:\n            chunks.append(\" \".join(current))\n            current = []\n    if current:\n        chunks.append(\" \".join(current))\n    \n    return chunks\n\nall_chunks = []\nfor doc in documents:\n    chunks = split_into_chunks(doc[\"content\"], max_words=300)\n    for i, chunk_text in enumerate(chunks):\n        all_chunks.append({\n            \"source\": doc[\"source\"],\n            \"text\": chunk_text,\n            \"chunk_id\": f\"{doc['source']}_chunk_{i}\"\n        })\n\nprint(f\"Total chunks created: {len(all_chunks)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:25:52.544842Z","iopub.execute_input":"2025-04-15T00:25:52.545207Z","iopub.status.idle":"2025-04-15T00:25:52.566954Z","shell.execute_reply.started":"2025-04-15T00:25:52.545170Z","shell.execute_reply":"2025-04-15T00:25:52.565589Z"}},"outputs":[{"name":"stdout","text":"Total chunks created: 219\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Embedding Generation\n\nIn this cell, I turned each 300‑word chunk into a semantic vector using Google’s GenAI embedding API. I started by defining `generate_embeddings()`, which takes a batch of texts and calls `genai.embed_content()` with the `text-embedding-004` model. If the response includes an `embedding` field, I return that list of vectors; otherwise I just loged an unexpected format or any errors and returned a list of `None` placeholders.\n\nTo keep the requests manageable, I looped over `all_chunks` in batches of 10, extracted the raw texts, and fed each batch into `generate_embeddings()` function. For each chunk that successfully returns an embedding vector, I built a new record including the original source, chunk text, chunk ID, and its embedding—and append it to `embedded_chunks`. At the end, I printed the total number of embedded chunks to verify that my semantic index is ready for similarity search.  \n","metadata":{}},{"cell_type":"code","source":"def generate_embeddings(batch_of_texts, embedding_model=\"models/text-embedding-004\"):\n    \"\"\"\n    Given a list of strings, returns a list of embedding vectors\n    using google.generativeai's embed_content method.\n    \"\"\"\n    try:\n        \n        response = genai.embed_content(content=batch_of_texts, model=embedding_model)      \n        if \"embedding\" in response:\n            return response[\"embedding\"]\n        else:\n            print(\"Unexpected response format:\", response)\n            return [None] * len(batch_of_texts)\n    except Exception as e:\n        print(f\"Embedding error: {e}\")\n        return [None] * len(batch_of_texts)\n\n\n\n\n# Embedding all chunks (in small batches to avoid large requests)\nembedded_chunks = []\nBATCH_SIZE = 10\n\nfor i in range(0, len(all_chunks), BATCH_SIZE):\n    batch = all_chunks[i:i+BATCH_SIZE]\n    texts = [item[\"text\"] for item in batch]\n    embeddings = generate_embeddings(texts)\n    \n    for item, emb in zip(batch, embeddings):\n        if emb is not None:\n            embedded_chunks.append({\n                \"source\": item[\"source\"],\n                \"text\": item[\"text\"],\n                \"chunk_id\": item[\"chunk_id\"],\n                \"embedding\": emb\n            })\n\nprint(f\"Successfully embedded {len(embedded_chunks)} chunks.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:27:33.037730Z","iopub.execute_input":"2025-04-15T00:27:33.038252Z","iopub.status.idle":"2025-04-15T00:27:39.031524Z","shell.execute_reply.started":"2025-04-15T00:27:33.038227Z","shell.execute_reply":"2025-04-15T00:27:39.030401Z"}},"outputs":[{"name":"stdout","text":"Successfully embedded 219 chunks.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Retrieval Augmented Generation (RAG) pipeline\n\nIn this cell, I implemented the core of the Retrieval‑Augmented Generation (RAG) pipeline: given a free‑form user query, I (1) embeded it into the same vector space as the document chunks, (2) computed cosine similarity between the query embedding and each chunk’s embedding, and (3) returned the **top _k_** most relevant chunks as context for generation.\n\n1. *Helper: `cosine_similarity(vec_a, vec_b)`* \n   Computes the cosine of the angle between two vectors which is a normalized dot product so that higher values which are closer to 1 indicate greater semantic similarity.\n\n2. *Function: `retrieve_top_k(query, data, k=3)`* \n   - Embed the query by calling `genai.embed_content()` with the same embedding model (`text-embedding-004`) used for the documents.  \n   - Score each chunk by pairing its precomputed embedding with the query embedding, then measuring similarity.  \n   - Sort and select the top _k_ chunks with the highest scores. These chunks are the most semantically related pieces of the rulebook to our question.\n\n3. *Quick Test*  \n   I then tried to run a sample query (“How do I set up the game board?”), which retrieved the top 3 chunks, and printed a preview of each to verify that our retrieval layer is correctly surfacing relevant rulebook passages.\n\nThis retrieval step ensures that before we hand anything to the text generation model, the vast rulebook is distilled into a few highly relevant excerpts making the subsequent answer both accurate and concise.  \n","metadata":{}},{"cell_type":"code","source":"import math\ndef cosine_similarity(vec_a, vec_b):\n    \"\"\"\n    Compute the cosine similarity between two vectors.\n    \"\"\"\n    dot = sum(a * b for a, b in zip(vec_a, vec_b))\n    mag_a = math.sqrt(sum(a * a for a in vec_a))\n    mag_b = math.sqrt(sum(b * b for b in vec_b))\n    return dot / (mag_a * mag_b) if (mag_a != 0 and mag_b != 0) else 0.0\n\ndef retrieve_top_k(query, data, k=3, embedding_model=\"models/text-embedding-004\"):\n    \"\"\"\n    1) Embed the query using genai.embed_content.\n    2) Compute cosine similarity with each chunk's embedding.\n    3) Return the top-k chunks.\n    \"\"\"\n    try:\n        response = genai.embed_content(content=[query], model=embedding_model)\n        if \"embedding\" in response:\n            query_emb = response[\"embedding\"][0]\n        else:\n            print(\"Unexpected response format when embedding query:\", response)\n            return []\n    except Exception as e:\n        print(f\"Embedding error in retrieve_top_k: {e}\")\n        return []\n\n    # Score each chunk\n    scored = []\n    for item in data:\n        sim = cosine_similarity(query_emb, item[\"embedding\"])\n        scored.append((item, sim))\n\n    # Sort by similarity in descending order\n    scored.sort(key=lambda x: x[1], reverse=True)\n    return [x[0] for x in scored[:k]]\n\n\n# Quick test\ntest_query = \"How do I set up the game board?\"\ntop_chunks = retrieve_top_k(test_query, embedded_chunks, k=3)\nfor i, chunk in enumerate(top_chunks, 1):\n    print(f\"[{i}] Source: {chunk['source']} | Preview: {chunk['text'][:200]}...\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:28:04.964230Z","iopub.execute_input":"2025-04-15T00:28:04.964577Z","iopub.status.idle":"2025-04-15T00:28:05.168887Z","shell.execute_reply.started":"2025-04-15T00:28:04.964552Z","shell.execute_reply":"2025-04-15T00:28:05.167951Z"}},"outputs":[{"name":"stdout","text":"[1] Source: Intarsia rulebook.pdf | Preview: 4 frames per color. In a 3-player game , remove 2 frames per color. In a 4-player game , remove no frames. In a 2-player game , return all tool tiles with a 3(+)-player icon or 4-player icon on their ...\n\n[2] Source: Intarsia rulebook.pdf | Preview: round, gain 1 point per connector on your floor board. If it’s the end of the 2nd round, gain 2 points per connector on your floor board. If it’s the end of the 3rd round, gain 3 points per connector ...\n\n[3] Source: PointCity rulebook.pdf | Preview: and place them in piles with their resource side faceup. Based on the number of players, remove/use the following number of cards from each tier: Return the removed cards to the game box, they will no...\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Text Generation with Gemini-2.0-Flash\n\nHere, the prompt is built by combining the retrieved context with the question, which is then passed to the Gemini-2.0-Flash text generation model. The output demonstrates Controlled Generation, using few-shot prompting wherein my prompt includes a few examples. This step shows how Gen AI can synthesize an answer from a given context.\n\n\nIn this cell, I implemented the synthesis phase of the RAG pipeline by taking the top‑K retrieved context chunks and the user’s query to produce a concise, accurate answer.\n\n1. *Context Assembly*  \n   I concatenated each retrieved chunk with its source filename and text preview into a single `context_text` block. This ensures the model sees all relevant rulebook excerpts at once.\n\n2. *Prompt Construction using Few‑Shot Prompting*  \n   I crafted a clear, few‑shot‑inspired prompt that:\n   - Frames the assistant as an expert on board game rules  \n   - Injects the concatenated context under a `CONTEXT:` header  \n   - Poses the user’s question under `QUESTION:`  \n   - Leaves an open `ANSWER:` slot for the model’s response  \n\nThis structure helps Gemini‑2.0‑Flash to stay focused on the given facts and deliver a directly relevant answer. Finally, I rendered the model’s response as a Markdown cell so it appears formatted and easy to read in the notebook.\n","metadata":{}},{"cell_type":"code","source":"def generate_answer(query, retrieved_context, text_model=\"gemini-2.0-flash\"):\n    \"\"\"\n    Use Google GenAI's generate_content to produce an answer to `query`\n    by incorporating the retrieved context.\n    \"\"\"\n    # Combine the retrieved context into one text block.\n    context_text = \"\\n\\n\".join(\n        [f\"Source: {c['source']}\\n{c['text']}\" for c in retrieved_context]\n    )\n    \n    # Build the prompt that includes context and the query using few shot prompting.\n    prompt = f\"\"\"\nYou are an expert on board game rules.\nUse the following context to answer the user's question accurately and concisely.\n\nCONTEXT:\n{context_text}\n\nQUESTION: {query}\n\nANSWER:\n\"\"\"\n    try:\n        answer = client.models.generate_content(\n            model=text_model,\n            contents=prompt\n        )\n        return answer.text  \n    except Exception as e:\n        return f\"Error generating text: {e}\"\n\n\nfinal_answer = generate_answer(\"How do I set up the game board?\", top_chunks)\nfrom IPython.display import Markdown\nMarkdown(f\"**Answer:** {final_answer}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:28:09.912078Z","iopub.execute_input":"2025-04-15T00:28:09.912453Z","iopub.status.idle":"2025-04-15T00:28:11.361330Z","shell.execute_reply.started":"2025-04-15T00:28:09.912430Z","shell.execute_reply":"2025-04-15T00:28:11.360345Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Answer:** To set up the game board for Intarsia, do the following:\n\n1.  Each player chooses a scoring marker and places it on space 0 of the score track on the score board.\n2.  Place one of the light brown markers as the reward marker onto the reward track space with the arrow.\n3.  Stack the 4 point tiles in the center of the score board.\n4.  Place the second light brown marker as the round marker onto space 1 of the round track.\n5.  Each player takes the floor board whose wall color matches their chosen scoring marker. Decide together which side to use (A or B). Then, each player places their floor board with that side facing up in front of them.\n6.  Each player takes 1 connector from the supply and places it in their floor board’s center.\n"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## Interactive QnA Loop\n\nIn this final cell, I tried to put everything together into a simple command‑line interface so users can ask arbitrary board game rule questions. For each query, I called the retrieval function to embed the user’s question with `text-embedding-004` model, then compute cosine similarities against our pre‑computed chunk embeddings, and pick the top 3 most relevant passages. Then, I passed those retrieved chunks plus the original question into my text generation function where Gemini‑2.0‑Flash helps to produce a concise, context‑grounded answer.\n\nThis loop makes our RAG pipeline fully interactive letting anyone ask “How do I set up the game board?”, “What happens on a tie?”, or any other rule query and get back a sub‑second, accurate answer drawn straight from the official PDF rulebooks.  \n","metadata":{}},{"cell_type":"code","source":"while True:\n    user_query = input(\"\\nEnter your board game rule question (or type 'exit'): \")\n    if user_query.lower() in [\"exit\", \"quit\"]:\n        break\n    \n   \n    context_chunks = retrieve_top_k(user_query, embedded_chunks, k=3)\n    \n    answer = generate_answer(user_query, context_chunks)\n    \n    print(\"\\nRetrieved Context (preview):\")\n    for idx, chunk in enumerate(context_chunks, 1):\n        print(f\"\\n[{idx}] Source: {chunk['source']} | Preview: {chunk['text'][:200]}...\")\n    print(\"\\nFinal Answer:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T00:28:16.800909Z","iopub.execute_input":"2025-04-15T00:28:16.801251Z","iopub.status.idle":"2025-04-15T00:29:03.667393Z","shell.execute_reply.started":"2025-04-15T00:28:16.801226Z","shell.execute_reply":"2025-04-15T00:29:03.666427Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"\nEnter your board game rule question (or type 'exit'):  chess rules\n"},{"name":"stdout","text":"\nRetrieved Context (preview):\n\n[1] Source: Chess rulebook.pdf | Preview: BASIC RULES OFCHESS Introduction Chess isagame ofstrategy believ edtohavebeen invented mor ethen 1500 years ago inIndia. Itisagame fortwo play - ers,one with the light pieces and one with the darkpiec...\n\n[2] Source: Chess rulebook.pdf | Preview: during agame and only when certain conditions aremet. Castling isaspecial move that lets aplay ermovetwo pieces atonce -theKingand one 2 Rook. Incastling, the play ermoveshisKing twosquar esei- ther t...\n\n[3] Source: Chess rulebook.pdf | Preview: one light-squar eBishop .The Bishop 'smoves areshownbythehighlighted squar esinthefollo wing chess- boar d.This black Bishop can captur ethewhite pawn but its path isblocked bytheblack Knight ifitwa...\n\nFinal Answer:\n Okay, I can help you with chess rules. Here's a summary of the basic rules, based on the provided text:\n\n**Objective:**\n\n*   The main goal of chess is to checkmate the opponent's King.\n\n**Board Setup:**\n\n*   The chessboard is eight squares long by eight squares wide.\n*   When sitting across from another player, the lighter color square goes on each player's right-hand side (\"light on right\").\n*   The white queen is placed on a white square, and the black queen is placed on a black square (\"queen on her own color\").\n*   The starting position of the pieces is as shown in the provided diagrams.\n\n**Basic Gameplay:**\n\n*   The player with the white pieces moves first.\n*   Players then take turns moving one piece at a time (except for castling).\n\n**Piece Movement:**\n\n*   **Queen:** Moves in a straight line any number of squares in any one direction - horizontal, vertical, or diagonal, as long as its path is not blocked by its own pieces. It can capture a piece of the opposite color in its path.\n*   **Rook:** Moves any number of squares in one direction - vertically or horizontally - if its path is not blocked.\n*   **Bishop:** Moves any number of squares diagonally if its path is not blocked. A bishop that starts on a light square can only move to other light squares, and a bishop that starts on a dark square can only move to other dark squares.\n*   **Knight:** Moves in an \"L\" shape - two squares horizontally or vertically, and then one square at a right angle (\"2 then 1\"). The Knight can jump over other pieces. The Knight always lands on a square opposite in color from its old square.\n*   **King:** Moves one square in any direction. The King may never move into check (a square attacked by an opponent's piece).\n*   **Pawn:** Moves straight ahead (never backward), but it captures diagonally. It moves one square at a time, but on its first move, it has the option of moving forward one or two squares. If a pawn advances all the way to the opposite end of the board, it is immediately promoted to another piece, usually a Queen (but not a King).\n\n**Special Moves:**\n\n*   **Castling:** A special move that allows a player to move two pieces at once - the King and one Rook. The player moves their King two squares either to its left or right toward one of their Rooks. At the same time, the Rook involved goes to the square on the other side of the King. Several conditions must be met to castle: neither the King nor the Rook involved may have moved before; the King may not castle out of check, into check, or through check; and there may not be pieces of either color between the King and the Rook involved.\n*   **En Passant:** A special pawn capture that can occur when a player moves a pawn two squares forward to avoid capture by an opponent's pawn. The opponent can capture the pawn \"in passing\" as if it had only moved one square forward, but only immediately after the two-square move.\n\nI hope this helps!\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter your board game rule question (or type 'exit'):  azul rules\n"},{"name":"stdout","text":"\nRetrieved Context (preview):\n\n[1] Source: Azul rulebook.pdf | Preview: Rulebook Center of the table FGame Setup 1. Give each player a player board (A). Flip your board to the side with the colored wall. (See Variant Play to play with the gray side of the player board). E...\n\n[2] Source: Azul rulebook.pdf | Preview: palace in Portugal be decorated with similar wall tiles. Azul brings you, a tile laying artist, to embellish the walls of the Royal Palace of Evora. A GAME BY MICHAEL KIESLING Wall Pattern lines Score...\n\n[3] Source: Eldritch rulebook.pdf | Preview: game if they solve three of the Ancient One’s Mysteries. If the Ancient One awakens, investigators will also need to solve the Final Mystery in order to win the game (see page 16). Azathoth Mystery Ca...\n\nFinal Answer:\n Azul is a tile-laying game where players compete to embellish the walls of the Royal Palace of Evora. Here's a breakdown of the gameplay:\n\n**Setup:**\n\n1.  Each player takes a player board and places their scoring marker on the \"0\" space of the score track.\n2.  The number of Factory displays placed in the center of the table depends on the number of players: 5 for 2 players, 7 for 3 players, and 9 for 4 players.\n3.  Fill the bag with 100 tiles (20 of each color).\n4.  The player who most recently visited Portugal takes the starting player marker and then fills each Factory display with exactly 4 tiles randomly drawn from the bag.\n\n**Gameplay:**\n\nThe game is played over multiple rounds, each with three phases:\n\n*   **Factory Offer:** The starting player places the starting player marker in the center of the table and then takes the first turn. Play continues clockwise. On your turn, you must pick tiles in one of the following ways:\n    *   Pick all tiles of the same color from any one Factory display and then move the remaining tiles from this Factory display to the center of the table.\n    *   Pick all tiles of the same color from the center of the table. If you are the first player in this round to pick tiles from the center of the table, also take the starting player marker and place it onto the leftmost free space of your floor line.\n*   **Wall-tiling:**\n*   **Preparing the next round**\n\n**Game End:**\n\nThe game ends after the round in which at least one player has completed a horizontal line of 5 consecutive tiles on their wall. The player with the most points at the end of the game wins.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter your board game rule question (or type 'exit'):  exit\n"}],"execution_count":14},{"cell_type":"markdown","source":"## Gen AI Capabilities Demonstrated\n\n### 1. Document Understanding\n- **PDF Extraction:** Uses PyPDF2 to automatically extract text from board game rule PDFs.  \n- **Chunking:** Splits long texts into manageable chunks to make retrieval more efficient and context‑relevant.\n\n### 2. Embeddings\n- **Semantic Vectors:** Generates embeddings via Google GenAI’s `embed_content` API (model: `text-embedding-004`), turning each text chunk into a high‑dimensional vector that captures its meaning.\n\n### 3. Retrieval Augmented Generation (RAG) & Vector Search\n- **Custom Retrieval:**  Implements a custom retrieval function that computes cosine similarity between a query embedding and each chunk's embedding, then retrieves the top-k most relevant chunks.\n- **In‑Memory Vector Search:** This simple in-memory retrieval strategy exemplifies vector search techniques.\n\n### 4. Controlled Generation (Few‑Shot Prompting)\n- **Structured Prompting:** Builds a clear, few‑shot prompt combining retrieved context with the user’s question.  \n- **GenAI Synthesis:** Sends the prompt to a text generation endpoint using Gemini‑2.0‑Flash via `client.models.generate_content` to produce a concise, accurate answer grounded in the rulebook.\n","metadata":{}},{"cell_type":"markdown","source":"# Project Summary\n\n## Innovation\n- **Real‑world Impact:** Helps new players rapidly understand complex board‑game rulebooks without manual page‑by‑page searching.  \n- **Integrated Pipeline:** The pipeline intelligently integrates document understanding, embeddings, and custom vector search to improve data retrieval, and controlled text generation to produce concise answers in seconds.\n\n## Gen AI Capabilities Demonstrated\n1. **Document Understanding**: Automated extraction and chunking of lengthy PDF rulebooks using PyPDF2.  \n2. **Embeddings & Vector Search**: Converting text chunks into semantic embeddings and retrieving them via cosine similarity based top‑k retrieval for in‑memory vector search.  \n3. **Retrieval Augmented Generation (RAG)**: Retrieved the most relevant context chunks to ground the answer.  \n4. **Controlled Generation (Few‑Shot Prompting)**: Constructing a detailed prompt that guides the text generation process.\n\n## Documentation Quality\n\nThe notebook clearly outlines the problem, describes the solution’s workflow, and explains every step with code and markdown. It communicates how each Gen AI capability contributes to solving the problem, ensuring that readers understand both the technical implementation and the innovation behind the solution.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}